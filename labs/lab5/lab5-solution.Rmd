---
title: "Business Analytics - ETC3250 2017 - Lab 5 solutions"
author: "Souhaib Ben Taieb"
date: "23 August 2017"
output: pdf_document
---

```{r, echo = FALSE, message = FALSE, warning = FALSE, warning = FALSE}
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  error = FALSE, 
  echo=T,
  collapse = TRUE,
  comment = "#",
  fig.height = 4,
  fig.width = 8,
  fig.align = "center",
  cache = FALSE
)
```

## Purpose

This lab will be on classification methods, including logistic regression, linear and quadratic discriminant analysis and nearest neighbors. 

Note: the online version of "An Introduction to Statistical Learning with Applications in R" (ISLR) is availalble at http://www-bcf.usc.edu/~gareth/ISL/.

### Exercice 1

Read and run the code in Sections 4.6.1 to 4.6.6 of ISLR.

### Assignment - Question 1

Do the exercise 7 in chapter 4.7 of ISLR.

Let $p_k(x)$ be the probability that a company will $(k = 1)$ or will not $(k = 0)$ issue a dividend this year given that its percentage profit was $x$ last year.

Using Bayes theorem and since we assume $X$ follows a normal distribution, we can write:

\[
p_k(x) = \frac {\pi_k
                \frac {1} {\sqrt{2 \pi} \sigma}
                \exp(- \frac {1} {2 \sigma^2} (x - \mu_k)^2)
               }
               {\sum_{l = 1}^k {
                \pi_l
                \frac {1} {\sqrt{2 \pi} \sigma}
                \exp(- \frac {1} {2 \sigma^2} (x - \mu_l)^2)
               }}, \quad k = 1, 2.
\]

Then, using $\pi_1 = .8$, $\sigma = 6$, $\mu_1 = 10$ and $\mu_2 = 0$, we have

\[
       p_{1}(x) = \frac {0.80 \exp(- \frac {1} {2 * 36}  (x - 10)^2)}
               {
                0.80 \exp(- \frac {1} {2 * 36} (x - 10)^2) +
                0.20 \exp(- \frac {1} {2 * 36}  x^2)
               }.
\]

Finally, since $x = 4$, we have

\[
p_{1}(4) \approx 75\%.
\]

### Assignment - Question 2

Do the exercise 8 in chapter 4.7 of ISLR.

$E_{\text{train}} = 0.20$, $E_{\text{test}} = 0.30$ and $E_{\text{avg}} = 0.25$.

$E_{\text{train}} = x_1$, $E_{\text{test}} = x_2$ and $E_{\text{avg}} = 0.18$.

I would prefer logistic regression since the test error rate is not too far from the training error rate. For 1-nearest neighbors, which is typically a high variance classifier, we could obtain an average error rate of $0.18$ with $x_1 = 0$ and $x_2 = 0.36$, which is an overfitting classifier with test error rate larger than $0.30$.

### Assignment - Question 3

Do the exercise 10 in chapter 4.7 of ISL

```{r echo=TRUE}
library(ISLR)
summary(Weekly)
pairs(Weekly)
attach(Weekly)
```

```{r echo=TRUE}
glm.fit <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Weekly, family = binomial)
summary(glm.fit)
```

```{r echo=TRUE}
glm.probs <- predict(glm.fit, type = "response")
glm.pred <- rep("Down", length(glm.probs))
glm.pred[glm.probs > 0.5] <- "Up"
table(glm.pred, Weekly$Direction)
```


```{r echo=TRUE}
train <- (Year <= 2008)
Weekly.test <- Weekly[!train, ]
glm.fit <- glm(Direction ~ Lag2, data = Weekly, family = binomial, subset = train)
glm.probs <- predict(glm.fit, Weekly.test, type = "response")
glm.pred <- rep("Down", length(glm.probs))
glm.pred[glm.probs > 0.5] <- "Up"
Direction.test <- Direction[!train]
table(glm.pred, Direction.test)
mean(glm.pred == Direction.test)
```



```{r echo=TRUE}
library(MASS)
lda.fit <- lda(Direction ~ Lag2, data = Weekly, subset = train)
lda.pred <- predict(lda.fit, Weekly.test)
table(lda.pred$class, Direction.test)
mean(lda.pred$class == Direction.test)
```

```{r echo=TRUE}
qda.fit <- qda(Direction ~ Lag2, data = Weekly, subset = train)
qda.class <- predict(qda.fit, Weekly.test)$class
table(qda.class, Direction.test)
mean(qda.class == Direction.test)
```

```{r echo=TRUE}
library(class)
train.X <- as.matrix(Lag2[train])
test.X <- as.matrix(Lag2[!train])
train.Direction = Direction[train]
knn.pred = knn(train.X, test.X, train.Direction, k = 100, prob = T)
table(knn.pred, Direction.test)
```


```{r echo=TRUE}
glm.fit <- glm(Direction ~ Lag2:Lag1, data = Weekly, family = binomial, subset = train)
glm.probs <- predict(glm.fit, Weekly.test, type = "response")
glm.pred <- rep("Down", length(glm.probs))
glm.pred[glm.probs > 0.5] <- "Up"
Direction.0910 <- Direction[!train]
table(glm.pred, Direction.test)
mean(glm.pred == Direction.test)
```

```{r echo=TRUE}
lda.fit = lda(Direction ~ Lag2:Lag1, data = Weekly, subset = train)
lda.pred = predict(lda.fit, Weekly.test)
mean(lda.pred$class == Direction.test)
```


```{r echo=TRUE}
for(k in seq(1, 100, by = 10)){
  knn.pred = knn(train.X, test.X, train.Direction, k = k, prob = T)
  print(table(knn.pred, Direction.test))
}
```

## TURN IN 

- Your `.Rmd` file (which should knit without errors and without assuming any packages have been pre-loaded)
- Your Word (or pdf) file that results from knitting the Rmd.
- DUE: August 27, 11:55pm (late submissions not allowed), loaded into moodle

